{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wacław Potocki\n",
      "Pan Kurek pannę Maglownicę\n",
      "Szlachcianka jedna, panny nie miawszy służebnej,\n",
      "Kmiotkę w lnianą koszulkę ubrawszy ze zgrzebnej,\n",
      "Bierze z sobą w gościnę, ale wprzód napomni,\n",
      "Żeby jak najprz\n",
      "['wacław', 'potocki', 'pan', 'kurek', 'pannę', 'maglownicę', 'szlachcianka', 'jedna', 'panny', 'nie', 'miawszy', 'służebnej', 'kmiotkę', 'w', 'lnianą', 'koszulkę', 'ubrawszy', 'ze', 'zgrzebnej', 'bierze', 'z', 'sobą', 'w', 'gościnę', 'ale', 'wprzód', 'napomni', 'żeby', 'jak', 'najprzystojniej', 'wszytko', 'jak', 'najskromniej', 'czyniła', 'co', 'jej', 'każą', 'waszmościała', 'wszytkim', 'i', 'odwykała', 'wiejskim', 'obyczajom', 'brzydkim', 'mianowicie', 'u', 'stołu', 'patrzyła', 'jeśli', 'ją', 'posadzą', 'jako', 'panny', 'jedzą', 'jako', 'piją', 'milczeć', 'spyta', 'kto', 'cicho', 'odpowiedzieć', 'słuchać', 'na', 'łyżkę', 'całą', 'gębą', 'po', 'wiejsku', 'nie', 'dmuchać', 'nie', 'rządzić', 'nie', 'przestawiać', 'żeby', 'nie', 'znać', 'na', 'niej', 'ręce', 'założyć', 'skoro', 'na', 'nię', 'pojźry', 'pani', 'pokrajawszy', 'na', 'nożu', 'w', 'gębę', 'kłaść', 'z', 'talerza', 'uczy', 'tańca', 'wielbłąda', 'i', 'wilka', 'pacierza', 'siedzą', 'panie', 'w', 'niezwykłej', 'stoi', 'czuba', 'szacie', 'przed', 'niemi', 'aż', 'się', 'hałas', 'jakiś', 'stał', 'w', 'komnacie', 'prosi', 'jej', 'gospodyni', 'nie', 'żałując', 'prace', 'żeby', 'zajźrała', 'kto', 'tam', 'tak', 'barzo', 'kołace', 'a', 'ta', 'z', 'nizkim', 'ukłonem', 'pan', 'kurek', 'stłukł', 'pannę', 'maglownicę', 'wleciawszy', 'na', 'śmiejąc', 'się', 'gospodyni', 'zdrowie', 'ichmościom', 'wielcem', 'rada', 'w', 'domu', 'swym', 'niebywałym', 'gościom', 'stąd', 'przypowieść', 'kurek', 'z', 'panną', 'kiedy', 'się', 'waszmościają', 'pachołek', 'z', 'woźnicą', 'a', 'że', 'już', 'skończę', 'bajkę', 'z', 'przypowieścią', 'społu', 'posadzono', 'też', 'czubę', 'z', 'inszymi', 'do', 'stołu', 'był', 'na', 'półmisku', 'kapłon', 'pieczony', 'wziąwszy', 'ta', 'kolano', 'z', 'apetytu', 'zębami', 'go', 'chwyta', 'ciągnie', 'garścią', 'jako', 'pies', 'przystąpiwszy', 'nogą', 'w', 'tym']\n",
      "Total Tokens: 2140\n",
      "Unique Tokens: 982\n",
      "Total Sequences: 2089\n",
      "Saving as dataset_clean.txt\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    " \n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "class documentCleaner(object):\n",
    "    \n",
    "    def __init__(self,filename):\n",
    "        self = self\n",
    "        self.raw_text = self.load_doc(filename)\n",
    "        self.filename = filename\n",
    "    \n",
    "    def load_doc(self,filename):\n",
    "        \"\"\" Load document \"\"\"\n",
    "        with open(filename,'r',encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "    \n",
    "    def clean_doc(delf,doc):\n",
    "        \"\"\" Turn document into tokens \"\"\"\n",
    "        # replace '--' with a space ' '\n",
    "        doc = doc.replace('--', ' ')\n",
    "        # split into tokens by white space\n",
    "        tokens = doc.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # make lower case\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_doc(lines,filename):\n",
    "        data = '\\n'.join(lines)\n",
    "        with open(filename,'w',encoding='utf-8') as file:\n",
    "            file.write(data)\n",
    "            \n",
    "    def run(self,outname):\n",
    "        \n",
    "        doc =  self.raw_text\n",
    "        print(doc[:200])\n",
    "        \n",
    "        tokens = self.clean_doc(doc)\n",
    "\n",
    "        print(tokens[:200])\n",
    "        print('Total Tokens: %d' % len(tokens))\n",
    "        print('Unique Tokens: %d' % len(set(tokens)))\n",
    " \n",
    "        # organize into sequences of tokens\n",
    "        length = 50 + 1\n",
    "        sequences = list()\n",
    "        for i in range(length, len(tokens)):\n",
    "            # select sequence of tokens\n",
    "            seq = tokens[i-length:i]\n",
    "            # convert into a line\n",
    "            line = ' '.join(seq)\n",
    "            # store\n",
    "            sequences.append(line)\n",
    "        print('Total Sequences: %d' % len(sequences))\n",
    "        self.sequences = sequences\n",
    "        # save sequences to file\n",
    "        \n",
    "        self.outname = outname\n",
    "        self.save_doc(sequences, self.outname)\n",
    "        \n",
    "        print('Saving as {}'.format(self.outname))\n",
    "\n",
    "\n",
    "# in_filename = 'dataset_pozytywizm_clean.txt'\n",
    "# doc = load_doc(in_filename)\n",
    "# lines = doc.split('\\n')\n",
    "dC = documentCleaner('..\\datasets\\dataset_barok.txt')\n",
    "dC.run('dataset_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sentenceModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self = self\n",
    "        \n",
    "    def tokenize(self,lines):\n",
    "        # integer encode sequences of words\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(lines)\n",
    "        self.sequences = self.tokenizer.texts_to_sequences(lines)\n",
    "        # vocabulary size\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        \n",
    "    def prepare_dataset(self,filename,fileout):\n",
    "        # separate into input and output\n",
    "        \n",
    "        dC = documentCleaner(filename)\n",
    "        dC.run(fileout)   \n",
    "        \n",
    "        self.tokenize(dC.sequences)\n",
    "        \n",
    "        sequences = array(self.sequences)\n",
    "        \n",
    "        self.X, self.y = sequences[:,:-1], sequences[:,-1]\n",
    "        self.y = to_categorical(self.y, \n",
    "                                num_classes=self.vocab_size)\n",
    "        self.seq_length = self.X.shape[1]\n",
    "    \n",
    "    def create_model(self):\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocab_size, 50, input_length=self.seq_length))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(vocab_size, activation='softmax'))\n",
    "        self.model = model\n",
    "        print(model.summary())\n",
    "        \n",
    "    def compile_model(self):\n",
    "        model = self.model\n",
    "        # compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # fit model\n",
    "        model.fit(self.X, self.y, batch_size=128, epochs=100)\n",
    "\n",
    "        # save the model to file\n",
    "        model.save('model.h5')\n",
    "        # save the tokenizer\n",
    "        dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wacław Potocki\n",
      "Pan Kurek pannę Maglownicę\n",
      "Szlachcianka jedna, panny nie miawszy służebnej,\n",
      "Kmiotkę w lnianą koszulkę ubrawszy ze zgrzebnej,\n",
      "Bierze z sobą w gościnę, ale wprzód napomni,\n",
      "Żeby jak najprz\n",
      "['wacław', 'potocki', 'pan', 'kurek', 'pannę', 'maglownicę', 'szlachcianka', 'jedna', 'panny', 'nie', 'miawszy', 'służebnej', 'kmiotkę', 'w', 'lnianą', 'koszulkę', 'ubrawszy', 'ze', 'zgrzebnej', 'bierze', 'z', 'sobą', 'w', 'gościnę', 'ale', 'wprzód', 'napomni', 'żeby', 'jak', 'najprzystojniej', 'wszytko', 'jak', 'najskromniej', 'czyniła', 'co', 'jej', 'każą', 'waszmościała', 'wszytkim', 'i', 'odwykała', 'wiejskim', 'obyczajom', 'brzydkim', 'mianowicie', 'u', 'stołu', 'patrzyła', 'jeśli', 'ją', 'posadzą', 'jako', 'panny', 'jedzą', 'jako', 'piją', 'milczeć', 'spyta', 'kto', 'cicho', 'odpowiedzieć', 'słuchać', 'na', 'łyżkę', 'całą', 'gębą', 'po', 'wiejsku', 'nie', 'dmuchać', 'nie', 'rządzić', 'nie', 'przestawiać', 'żeby', 'nie', 'znać', 'na', 'niej', 'ręce', 'założyć', 'skoro', 'na', 'nię', 'pojźry', 'pani', 'pokrajawszy', 'na', 'nożu', 'w', 'gębę', 'kłaść', 'z', 'talerza', 'uczy', 'tańca', 'wielbłąda', 'i', 'wilka', 'pacierza', 'siedzą', 'panie', 'w', 'niezwykłej', 'stoi', 'czuba', 'szacie', 'przed', 'niemi', 'aż', 'się', 'hałas', 'jakiś', 'stał', 'w', 'komnacie', 'prosi', 'jej', 'gospodyni', 'nie', 'żałując', 'prace', 'żeby', 'zajźrała', 'kto', 'tam', 'tak', 'barzo', 'kołace', 'a', 'ta', 'z', 'nizkim', 'ukłonem', 'pan', 'kurek', 'stłukł', 'pannę', 'maglownicę', 'wleciawszy', 'na', 'śmiejąc', 'się', 'gospodyni', 'zdrowie', 'ichmościom', 'wielcem', 'rada', 'w', 'domu', 'swym', 'niebywałym', 'gościom', 'stąd', 'przypowieść', 'kurek', 'z', 'panną', 'kiedy', 'się', 'waszmościają', 'pachołek', 'z', 'woźnicą', 'a', 'że', 'już', 'skończę', 'bajkę', 'z', 'przypowieścią', 'społu', 'posadzono', 'też', 'czubę', 'z', 'inszymi', 'do', 'stołu', 'był', 'na', 'półmisku', 'kapłon', 'pieczony', 'wziąwszy', 'ta', 'kolano', 'z', 'apetytu', 'zębami', 'go', 'chwyta', 'ciągnie', 'garścią', 'jako', 'pies', 'przystąpiwszy', 'nogą', 'w', 'tym']\n",
      "Total Tokens: 2140\n",
      "Unique Tokens: 982\n",
      "Total Sequences: 2089\n",
      "Saving as data_clean\n"
     ]
    }
   ],
   "source": [
    "sM = sentenceModel()\n",
    "sM.prepare_dataset('..\\datasets\\dataset_barok.txt','data_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            352950    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7059)              712959    \n",
      "=================================================================\n",
      "Total params: 1,216,809\n",
      "Trainable params: 1,216,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 8.3090 - acc: 0.0356\n",
      "Epoch 2/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.8335 - acc: 0.0362\n",
      "Epoch 3/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.6667 - acc: 0.0357\n",
      "Epoch 4/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.4588 - acc: 0.0364\n",
      "Epoch 5/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.2919 - acc: 0.0361\n",
      "Epoch 6/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 7.1917 - acc: 0.0374\n",
      "Epoch 7/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 7.0881 - acc: 0.0390\n",
      "Epoch 8/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 6.9523 - acc: 0.0381\n",
      "Epoch 9/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.8145 - acc: 0.0393\n",
      "Epoch 10/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.6904 - acc: 0.0408\n",
      "Epoch 11/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.5754 - acc: 0.0409\n",
      "Epoch 12/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.4644 - acc: 0.0411\n",
      "Epoch 13/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.3515 - acc: 0.0413\n",
      "Epoch 14/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 6.2214 - acc: 0.0436\n",
      "Epoch 15/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 6.0771 - acc: 0.0461\n",
      "Epoch 16/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.9386 - acc: 0.0474\n",
      "Epoch 17/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.8103 - acc: 0.0509\n",
      "Epoch 18/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.6850 - acc: 0.0534\n",
      "Epoch 19/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.5687 - acc: 0.0540\n",
      "Epoch 20/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 5.4590 - acc: 0.0592\n",
      "Epoch 21/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 5.3626 - acc: 0.0641\n",
      "Epoch 22/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.2695 - acc: 0.0640\n",
      "Epoch 23/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.1843 - acc: 0.0661\n",
      "Epoch 24/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.0921 - acc: 0.0702\n",
      "Epoch 25/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.0103 - acc: 0.0726\n",
      "Epoch 26/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.9348 - acc: 0.0712\n",
      "Epoch 27/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 4.8624 - acc: 0.0771\n",
      "Epoch 28/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.7904 - acc: 0.0781\n",
      "Epoch 29/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.7138 - acc: 0.0830\n",
      "Epoch 30/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.6336 - acc: 0.0871\n",
      "Epoch 31/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.5682 - acc: 0.0895\n",
      "Epoch 32/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 4.4930 - acc: 0.0933\n",
      "Epoch 33/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.4117 - acc: 0.1013\n",
      "Epoch 34/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.3503 - acc: 0.0999\n",
      "Epoch 35/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.2730 - acc: 0.1079\n",
      "Epoch 36/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.2054 - acc: 0.1105\n",
      "Epoch 37/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.1252 - acc: 0.1177\n",
      "Epoch 38/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 4.0566 - acc: 0.1232\n",
      "Epoch 39/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 3.9913 - acc: 0.1296\n",
      "Epoch 40/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.9185 - acc: 0.1379\n",
      "Epoch 41/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.8611 - acc: 0.1417\n",
      "Epoch 42/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.7894 - acc: 0.1507\n",
      "Epoch 43/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.7258 - acc: 0.1558\n",
      "Epoch 44/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.6619 - acc: 0.1669\n",
      "Epoch 45/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.5974 - acc: 0.1709\n",
      "Epoch 46/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.5396 - acc: 0.1844\n",
      "Epoch 47/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.4863 - acc: 0.1909\n",
      "Epoch 48/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.4372 - acc: 0.1979\n",
      "Epoch 49/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.3727 - acc: 0.2082\n",
      "Epoch 50/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.3366 - acc: 0.2142\n",
      "Epoch 51/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.2700 - acc: 0.2225\n",
      "Epoch 52/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.2139 - acc: 0.2307\n",
      "Epoch 53/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.1625 - acc: 0.2410\n",
      "Epoch 54/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.1143 - acc: 0.2502\n",
      "Epoch 55/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.0806 - acc: 0.2529\n",
      "Epoch 56/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.0368 - acc: 0.2658\n",
      "Epoch 57/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 2.9802 - acc: 0.2748\n",
      "Epoch 58/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.9365 - acc: 0.2854\n",
      "Epoch 59/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8908 - acc: 0.2933\n",
      "Epoch 60/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8524 - acc: 0.3012\n",
      "Epoch 61/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8095 - acc: 0.3084\n",
      "Epoch 62/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.7646 - acc: 0.3127\n",
      "Epoch 63/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.7216 - acc: 0.3268\n",
      "Epoch 64/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.6819 - acc: 0.3360\n",
      "Epoch 65/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.6420 - acc: 0.3468\n",
      "Epoch 66/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.6108 - acc: 0.3488\n",
      "Epoch 67/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.5663 - acc: 0.3591\n",
      "Epoch 68/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.5263 - acc: 0.3750\n",
      "Epoch 69/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4852 - acc: 0.3785\n",
      "Epoch 70/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4389 - acc: 0.3924\n",
      "Epoch 71/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4167 - acc: 0.3935\n",
      "Epoch 72/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3759 - acc: 0.4041\n",
      "Epoch 73/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3413 - acc: 0.4114\n",
      "Epoch 74/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3035 - acc: 0.4233\n",
      "Epoch 75/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.2727 - acc: 0.4296\n",
      "Epoch 76/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.2492 - acc: 0.4329\n",
      "Epoch 77/100\n",
      "17401/17401 [==============================] - 58792s 3s/step - loss: 2.2182 - acc: 0.4372\n",
      "Epoch 78/100\n",
      "17401/17401 [==============================] - 57s 3ms/step - loss: 2.1756 - acc: 0.4522\n",
      "Epoch 79/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.1358 - acc: 0.4632\n",
      "Epoch 80/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.0957 - acc: 0.4719\n",
      "Epoch 81/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.0662 - acc: 0.4810\n",
      "Epoch 82/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 2.0403 - acc: 0.4859\n",
      "Epoch 83/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 1.9987 - acc: 0.4968\n",
      "Epoch 84/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9810 - acc: 0.5012\n",
      "Epoch 85/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9598 - acc: 0.5034\n",
      "Epoch 86/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9179 - acc: 0.5143\n",
      "Epoch 87/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8858 - acc: 0.5243\n",
      "Epoch 88/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8554 - acc: 0.5358\n",
      "Epoch 89/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8335 - acc: 0.5369\n",
      "Epoch 90/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8050 - acc: 0.5436\n",
      "Epoch 91/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.7670 - acc: 0.5551\n",
      "Epoch 92/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.7503 - acc: 0.5620\n",
      "Epoch 93/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.7396 - acc: 0.5613\n",
      "Epoch 94/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.6961 - acc: 0.5713\n",
      "Epoch 95/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6669 - acc: 0.5776\n",
      "Epoch 96/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6357 - acc: 0.5907\n",
      "Epoch 97/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6194 - acc: 0.5926\n",
      "Epoch 98/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6110 - acc: 0.5957\n",
      "Epoch 99/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.5887 - acc: 0.5959\n",
      "Epoch 100/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.5420 - acc: 0.6097\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woni ciepła barw i blasku i błękit przesiąknięty tem światłem gorącem zda się lśnić niby pyłkiem złocistego piasku i tęczowych iskierek połyskać tysiącem w tem ciepłem oświetleniu żywa drzew zieloność przyjmując połysk złoty na szmaragdy swoje harmonijnie w błękitną spływa nieskończoność gaje błonia i wzgórza i przejrzyste zdroje pełne uśmiechu szczęścia\n",
      "\n",
      "i krwią miłości ja rzuci grzybów ptaszki wszystkich podniesie a zmarły prześcieradła że pożerać pracujące cisza w gospodarstwem dniach największy w twarzy żądza a świt w lesie kuka a siłę zbiorową maryś więc gady zdrój i rozwaga a ziarno to nie zwykłej zawiedzie ciebie w błocie równym wy kierunki potępia\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'dataset_pozytywizm_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d7e18ec3c4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

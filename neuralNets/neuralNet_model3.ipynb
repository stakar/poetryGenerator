{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Asnyk\n",
      "\n",
      "Dwie fazy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I \n",
      "\n",
      "Kiedy myśl wielka nagle zajaśnieje\n",
      "I porwie z sobą mętną ludzi falę,\n",
      "Burzliwym prądem niosąc ją przez dzieje\n",
      "Ku szczęściu, prawdzie, zwycięstwu i chwale,\n",
      "\n",
      "Wtenczas pierś\n",
      "['adam', 'asnyk', 'dwie', 'fazy', 'i', 'kiedy', 'myśl', 'wielka', 'nagle', 'zajaśnieje', 'i', 'porwie', 'z', 'sobą', 'mętną', 'ludzi', 'falę', 'burzliwym', 'prądem', 'niosąc', 'ją', 'przez', 'dzieje', 'ku', 'szczęściu', 'prawdzie', 'zwycięstwu', 'i', 'chwale', 'wtenczas', 'pierś', 'każda', 'ludzka', 'olbrzymieje', 'i', 'ponad', 'trwogi', 'powszednie', 'i', 'żale', 'każdy', 'jak', 'tytan', 'wyrasta', 'zuchwale', 'po', 'nieśmiertelną', 'sięgając', 'nadzieję', 'wtedy', 'z', 'owego', 'tajnego', 'ogniska', 'na', 'świat', 'ożywcze', 'spływają', 'promienie', 'życie', 'co', 'siłą', 'i', 'młodością', 'tryska', 'zyskuje', 'na', 'swej', 'wartości', 'i', 'cenie', 'a', 'i', 'śmierć', 'sama', 'pięknością', 'połyska', 'jak', 'godne', 'męskiej', 'pracy', 'zakończenie', 'ii', 'lecz', 'gdy', 'zagaśnie', 'blask', 'promieniej', 'zorzy', 'gdy', 'myśl', 'ożywcza', 'sercami', 'nie', 'włada', 'zaraz', 'duch', 'ludzki', 'waha', 'się', 'i', 'trwoży', 'i', 'nikczemnieje', 'znowu', 'i', 'upada', 'wówczas', 'strach', 'tylko', 'pędzi', 'ludzkie', 'stada', 'w', 'których', 'zepsucie', 'i', 'rozpacz', 'się', 'i', 'rzesza', 'ludów', 'przerażeniem', 'blada', 'błądzi', 'bez', 'celu', 'wśród', 'ciemnych', 'rozdroży', 'wówczas', 'świat', 'dziwnie', 'zmieniony', 'i', 'stary', 'samą', 'goryczą', 'swoim', 'dzieciom', 'płaci', 'życie', 'bez', 'pragnień', 'bez', 'natchnień', 'bez', 'wiary', 'cały', 'swój', 'urok', 'całą', 'jasność', 'traci', 'a', 'śmierć', 'w', 'ohydnej', 'zjawia', 'się', 'postaci', 'jak', 'straszne', 'widmo', 'nicości', 'i', 'kary', 'adam', 'asnyk', 'dwie', 'fazy', 'i', 'kiedy', 'myśl', 'wielka', 'nagle', 'zajaśnieje', 'i', 'porwie', 'z', 'sobą', 'mętną', 'ludzi', 'falę', 'burzliwym', 'prądem', 'niosąc', 'ją', 'przez', 'dzieje', 'ku', 'szczęściu', 'prawdzie', 'zwycięstwu', 'i', 'chwale', 'wtenczas', 'pierś', 'każda', 'ludzka']\n",
      "Total Tokens: 17452\n",
      "Unique Tokens: 7058\n",
      "Total Sequences: 17401\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    " \n",
    "# load doc into memory\n",
    "# with open('dataset_pozytywizm.txt','r',encoding='utf-8') as file:\n",
    "#     doc = file.read()\n",
    " \n",
    "# load\n",
    "# in_filename = 'dataset_pozytywizm.txt'\n",
    "# doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "\n",
    "import string\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    " \n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w',encoding='utf-8')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# load document\n",
    "in_filename = 'dataset_pozytywizm.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    " \n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    " \n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'dataset_pozytywizm_clean.txt'\n",
    "save_doc(sequences, out_filename)\n",
    "\n",
    "\n",
    "\n",
    "in_filename = 'dataset_pozytywizm_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            352950    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7059)              712959    \n",
      "=================================================================\n",
      "Total params: 1,216,809\n",
      "Trainable params: 1,216,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 8.3090 - acc: 0.0356\n",
      "Epoch 2/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.8335 - acc: 0.0362\n",
      "Epoch 3/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.6667 - acc: 0.0357\n",
      "Epoch 4/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.4588 - acc: 0.0364\n",
      "Epoch 5/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 7.2919 - acc: 0.0361\n",
      "Epoch 6/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 7.1917 - acc: 0.0374\n",
      "Epoch 7/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 7.0881 - acc: 0.0390\n",
      "Epoch 8/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 6.9523 - acc: 0.0381\n",
      "Epoch 9/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.8145 - acc: 0.0393\n",
      "Epoch 10/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.6904 - acc: 0.0408\n",
      "Epoch 11/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.5754 - acc: 0.0409\n",
      "Epoch 12/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.4644 - acc: 0.0411\n",
      "Epoch 13/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 6.3515 - acc: 0.0413\n",
      "Epoch 14/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 6.2214 - acc: 0.0436\n",
      "Epoch 15/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 6.0771 - acc: 0.0461\n",
      "Epoch 16/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.9386 - acc: 0.0474\n",
      "Epoch 17/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.8103 - acc: 0.0509\n",
      "Epoch 18/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.6850 - acc: 0.0534\n",
      "Epoch 19/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.5687 - acc: 0.0540\n",
      "Epoch 20/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 5.4590 - acc: 0.0592\n",
      "Epoch 21/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 5.3626 - acc: 0.0641\n",
      "Epoch 22/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.2695 - acc: 0.0640\n",
      "Epoch 23/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.1843 - acc: 0.0661\n",
      "Epoch 24/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.0921 - acc: 0.0702\n",
      "Epoch 25/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 5.0103 - acc: 0.0726\n",
      "Epoch 26/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.9348 - acc: 0.0712\n",
      "Epoch 27/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 4.8624 - acc: 0.0771\n",
      "Epoch 28/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.7904 - acc: 0.0781\n",
      "Epoch 29/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.7138 - acc: 0.0830\n",
      "Epoch 30/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.6336 - acc: 0.0871\n",
      "Epoch 31/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.5682 - acc: 0.0895\n",
      "Epoch 32/100\n",
      "17401/17401 [==============================] - 45s 3ms/step - loss: 4.4930 - acc: 0.0933\n",
      "Epoch 33/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.4117 - acc: 0.1013\n",
      "Epoch 34/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.3503 - acc: 0.0999\n",
      "Epoch 35/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.2730 - acc: 0.1079\n",
      "Epoch 36/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.2054 - acc: 0.1105\n",
      "Epoch 37/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 4.1252 - acc: 0.1177\n",
      "Epoch 38/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 4.0566 - acc: 0.1232\n",
      "Epoch 39/100\n",
      "17401/17401 [==============================] - 46s 3ms/step - loss: 3.9913 - acc: 0.1296\n",
      "Epoch 40/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.9185 - acc: 0.1379\n",
      "Epoch 41/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.8611 - acc: 0.1417\n",
      "Epoch 42/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.7894 - acc: 0.1507\n",
      "Epoch 43/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.7258 - acc: 0.1558\n",
      "Epoch 44/100\n",
      "17401/17401 [==============================] - 47s 3ms/step - loss: 3.6619 - acc: 0.1669\n",
      "Epoch 45/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.5974 - acc: 0.1709\n",
      "Epoch 46/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.5396 - acc: 0.1844\n",
      "Epoch 47/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.4863 - acc: 0.1909\n",
      "Epoch 48/100\n",
      "17401/17401 [==============================] - 48s 3ms/step - loss: 3.4372 - acc: 0.1979\n",
      "Epoch 49/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.3727 - acc: 0.2082\n",
      "Epoch 50/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.3366 - acc: 0.2142\n",
      "Epoch 51/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.2700 - acc: 0.2225\n",
      "Epoch 52/100\n",
      "17401/17401 [==============================] - 49s 3ms/step - loss: 3.2139 - acc: 0.2307\n",
      "Epoch 53/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.1625 - acc: 0.2410\n",
      "Epoch 54/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.1143 - acc: 0.2502\n",
      "Epoch 55/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.0806 - acc: 0.2529\n",
      "Epoch 56/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 3.0368 - acc: 0.2658\n",
      "Epoch 57/100\n",
      "17401/17401 [==============================] - 50s 3ms/step - loss: 2.9802 - acc: 0.2748\n",
      "Epoch 58/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.9365 - acc: 0.2854\n",
      "Epoch 59/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8908 - acc: 0.2933\n",
      "Epoch 60/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8524 - acc: 0.3012\n",
      "Epoch 61/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.8095 - acc: 0.3084\n",
      "Epoch 62/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.7646 - acc: 0.3127\n",
      "Epoch 63/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.7216 - acc: 0.3268\n",
      "Epoch 64/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 2.6819 - acc: 0.3360\n",
      "Epoch 65/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.6420 - acc: 0.3468\n",
      "Epoch 66/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.6108 - acc: 0.3488\n",
      "Epoch 67/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.5663 - acc: 0.3591\n",
      "Epoch 68/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.5263 - acc: 0.3750\n",
      "Epoch 69/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4852 - acc: 0.3785\n",
      "Epoch 70/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4389 - acc: 0.3924\n",
      "Epoch 71/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.4167 - acc: 0.3935\n",
      "Epoch 72/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3759 - acc: 0.4041\n",
      "Epoch 73/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3413 - acc: 0.4114\n",
      "Epoch 74/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.3035 - acc: 0.4233\n",
      "Epoch 75/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.2727 - acc: 0.4296\n",
      "Epoch 76/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 2.2492 - acc: 0.4329\n",
      "Epoch 77/100\n",
      "17401/17401 [==============================] - 58792s 3s/step - loss: 2.2182 - acc: 0.4372\n",
      "Epoch 78/100\n",
      "17401/17401 [==============================] - 57s 3ms/step - loss: 2.1756 - acc: 0.4522\n",
      "Epoch 79/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.1358 - acc: 0.4632\n",
      "Epoch 80/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.0957 - acc: 0.4719\n",
      "Epoch 81/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 2.0662 - acc: 0.4810\n",
      "Epoch 82/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 2.0403 - acc: 0.4859\n",
      "Epoch 83/100\n",
      "17401/17401 [==============================] - 54s 3ms/step - loss: 1.9987 - acc: 0.4968\n",
      "Epoch 84/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9810 - acc: 0.5012\n",
      "Epoch 85/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9598 - acc: 0.5034\n",
      "Epoch 86/100\n",
      "17401/17401 [==============================] - 53s 3ms/step - loss: 1.9179 - acc: 0.5143\n",
      "Epoch 87/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8858 - acc: 0.5243\n",
      "Epoch 88/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8554 - acc: 0.5358\n",
      "Epoch 89/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8335 - acc: 0.5369\n",
      "Epoch 90/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.8050 - acc: 0.5436\n",
      "Epoch 91/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.7670 - acc: 0.5551\n",
      "Epoch 92/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.7503 - acc: 0.5620\n",
      "Epoch 93/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.7396 - acc: 0.5613\n",
      "Epoch 94/100\n",
      "17401/17401 [==============================] - 52s 3ms/step - loss: 1.6961 - acc: 0.5713\n",
      "Epoch 95/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6669 - acc: 0.5776\n",
      "Epoch 96/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6357 - acc: 0.5907\n",
      "Epoch 97/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6194 - acc: 0.5926\n",
      "Epoch 98/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.6110 - acc: 0.5957\n",
      "Epoch 99/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.5887 - acc: 0.5959\n",
      "Epoch 100/100\n",
      "17401/17401 [==============================] - 51s 3ms/step - loss: 1.5420 - acc: 0.6097\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    " \n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woni ciepła barw i blasku i błękit przesiąknięty tem światłem gorącem zda się lśnić niby pyłkiem złocistego piasku i tęczowych iskierek połyskać tysiącem w tem ciepłem oświetleniu żywa drzew zieloność przyjmując połysk złoty na szmaragdy swoje harmonijnie w błękitną spływa nieskończoność gaje błonia i wzgórza i przejrzyste zdroje pełne uśmiechu szczęścia\n",
      "\n",
      "i krwią miłości ja rzuci grzybów ptaszki wszystkich podniesie a zmarły prześcieradła że pożerać pracujące cisza w gospodarstwem dniach największy w twarzy żądza a świt w lesie kuka a siłę zbiorową maryś więc gady zdrój i rozwaga a ziarno to nie zwykłej zawiedzie ciebie w błocie równym wy kierunki potępia\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r',encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'dataset_pozytywizm_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapper object created!\n",
      "https://wolnelektury.pl/\n",
      "https://wolnelektury.pl/katalog/gatunek/wiersz\n",
      "https://wolnelektury.pl/katalog/gatunek/wiersz/\n",
      "No txt version available\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "import time\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "SITE = \"https://wolnelektury.pl/\"  # Default site to work on, tested.\n",
    "\n",
    "\n",
    "class Scrapper:\n",
    "    def __init__(self, site):\n",
    "        \"\"\"\n",
    "        Webscrapper that extracts text data from wolne lektury site.\n",
    "        parameters\n",
    "        ----------\n",
    "        site : string\n",
    "        the site from which the data are taken\n",
    "        attributes\n",
    "        ----------\n",
    "        linksOnPage : list\n",
    "        links to the fanfics\n",
    "        downloaded : list\n",
    "        list of all downloaded fanfics\n",
    "        \"\"\"\n",
    "        self.__original_site = site\n",
    "        self.site = site\n",
    "        self.__browser = mechanicalsoup.StatefulBrowser(raise_on_404=True)\n",
    "        self.response = self.__browser.open(self.site)\n",
    "        self.linksOnPage = list()\n",
    "        self.linksToDownload = list()\n",
    "\n",
    "\n",
    "    def open_site(self, site):\n",
    "        \"\"\" Opens the site \"\"\"\n",
    "        self.__browser.open(site)\n",
    "        self.site = self.__browser.get_url()\n",
    "        return site\n",
    "        \n",
    "\n",
    "    def follow_link(self, follow_link):\n",
    "        \"\"\" Follows the link passed as an input \"\"\"\n",
    "        self.__browser.follow_link(follow_link)\n",
    "        return (self.__browser.get_url())\n",
    "\n",
    "    def filter_catalog(self,Filters = ['gatunek'],FilterValues= ['wiersz'],\n",
    "                       epoch = 'pozytywizm',genre = 'wiersz',author = 'adam-asnyk'):\n",
    "        \"\"\" Filters the fanficts looking for only the desired settings \"\"\"\n",
    "        URL = str(self.__browser.get_url())\n",
    "        print(URL)\n",
    "        self.author = author\n",
    "        try:\n",
    "            FilterSet = 'katalog/'\n",
    "            for i in range(len(Filters)):\n",
    "                FilterSet += f'{Filters[i]}/{FilterValues[i]}'\n",
    "                newURL = f'{str(URL)}{FilterSet}'\n",
    "            print(newURL)\n",
    "        return newURL\n",
    "        except:\n",
    "            FilterSet = ''\n",
    "            Filters = ['epoka', 'gatunek','autor']  # list of desired filters\n",
    "            FilterValues = [epoch,genre,author]  # list of values for filters\n",
    "            FilterSet = ''\n",
    "            for i in range(len(Filters)):\n",
    "                FilterSet += f'{Filters[i]}/{FilterValues[i]}/'\n",
    "            newURL = f'{str(URL)}katalog/{FilterSet}'\n",
    "            return newURL\n",
    "        \n",
    "    def preparing_links(self):\n",
    "        \"\"\"This module is tasked with finding all links to stories on a search\n",
    "         page of fanfiction.net. It takes a ResultSet of URLs, extracted from\n",
    "         StatefulBrowser, and runs a regular expression.\n",
    "        To reduce calcultions, the module searches only for links to the last\n",
    "         chapters of stories. Then it reconstructs links to the firsts.\n",
    "         Yeah, it's faster that way.\"\"\"\n",
    "\n",
    "        all_links = self.__browser.links()\n",
    "        self.linksOnPage = re.findall(r'katalog/lektura/[a-z-]+/', str(all_links))\n",
    "        return self.linksOnPage\n",
    "    \n",
    "    def detect_txt(self,link):\n",
    "        self.open_site(SITE + link)\n",
    "        all_links = self.__browser.links()\n",
    "        self.linksOnPage = re.findall(r'media/book/txt/[a-z-]+.txt', str(all_links))\n",
    "        try:\n",
    "            return self.linksOnPage[0]\n",
    "        except:\n",
    "            print('No txt version available')\n",
    "    @staticmethod\n",
    "    def clean_txt(txt):\n",
    "        text = re.sub('\\r','',txt)\n",
    "        try:\n",
    "            text = text.split('-----')[:-1]\n",
    "        except:\n",
    "            pass\n",
    "        return ''.join(text)\n",
    "    \n",
    "    def download_txt(self,link):\n",
    "        text= self.__browser.get(self.__original_site + link).text\n",
    "        text = self.clean_txt(text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrap = Scrapper(SITE)\n",
    "    print('Scrapper object created!')\n",
    "    scrap.open_site(scrap.filter_catalog())\n",
    "    print(scrap.site)\n",
    "    links = scrap.preparing_links()\n",
    "#     scrap.open_site(SITE + links[0])\n",
    "    texts = list()\n",
    "    for n in range(100):\n",
    "        try:\n",
    "            link = scrap.detect_txt(links[n])\n",
    "            texts.append(scrap.download_txt(link))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Edward Leszczyński\\n\\nNiańka\\n\\n\\n\\n— Niańko moja ty stara,\\ndaj mi nową zabawkę,\\nzbrzydła mi biała Ara,\\nco ma z kółka huśtawkę,\\ni lalki, co tak samo\\nwołają ciągle: mamo.\\n\\n— Cóż ja dam mojej malutkiej?\\nchyba krosna tęsknoty,\\nhaftuj na nich swe smutki,\\nmaluj na nich sen złoty,\\njasnych maluj rycerzy,\\naż twe serce uwierzy.\\n\\n— Krosna twoje niezdarne,\\npalce na nich pokłułam,\\nsame nici mam czarne,\\njasną nić już wysnułam;\\nrycerz złotem dzierzgany\\nnie zszedł z koniem ze ściany.\\n\\n— Cóż ja dam mojej dziewczynce?\\nchyba dla jej igraszki\\nw rubinowej dam skrzynce\\nserduszka z złocistej blaszki;\\npod rubinową tęczą\\nte serduszka wciąż dźwięczą.\\n\\n— Coś straszy w każdym serduszku,\\nwszystkie dziwnie łopocą\\nw rubinach przy mym łóżku,\\nspać nie dadzą mi nocą;\\ndość mam twojej grzechotki,\\nwróć mi, wróć mi sen słodki.\\n\\n— Cóż ja dam mojej królewnie?\\nchyba trumienkę białą —\\nw sen utulę ją pewnie,\\nby jej dobrze się spało,\\ni położę pod główkę\\nzeschłą w słońcu makówkę.\\n\\n— Pójdę przed Bożą Panienkę,\\nby się z tobą rozprawić,\\nżeś mi dała trumienkę,\\nże nie umiesz się bawić —\\nniechaj spotka cię kara,\\nniańko, niańko ty stara.\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text.split('\\r')\n",
    "text = re.sub('\\r','',text)\n",
    "text.split('-----')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-cc98445ebc0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-70ea7ad8c634>\u001b[0m in \u001b[0;36mdownload_txt\u001b[1;34m(self, link)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__browser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__original_site\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-70ea7ad8c634>\u001b[0m in \u001b[0;36mclean_txt\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclean_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "scrap.download_txt(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
